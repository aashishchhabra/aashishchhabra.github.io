[ { "title": "Deploying Apache Kafka Part 1", "url": "/posts/deploying-apache-kafka-part-1/", "categories": "Knowledge Base", "tags": "kafka, cluster setup", "date": "2022-04-21 00:08:00 +0530", "snippet": "What is Apache KafkaApache Kafka is an open source event streaming tool that is similar to multiple publish / subscribe tools available but its popularity has been growing due to multiple large scale organizations using Kafka to drive their event driven systems. This article won’t be covering details of Apache Kafka and will concentrate on deploying Apache kafka on linux systems in cluster mode with some basic configurations.Setting up the infrastructureI will be using Google Cloud VM instances to create a 3 node cluster setup with 1 node for ansible controller which can be easily done even if you have Free Credits from Google. Steps for creating a GCP account with free credits can be easily found on multiple websites.Below is the mentioned configuration of VM instances (total 4 VMs):Machine Series: N2Machine Type: n2-standard-2vCPU: 2Memory: 8 GBOperating System: CentOS 7Once you have 4 VM instances deployed either on GCP or platform of your choice, lets start with some steps: Setup Passwordless SSH from ansible controller node to other nodes. Install Ansible controller node ssh node4yum install -y ansible Create hosts file for ansible inventory as below [kafka]node1node2node3[zoo]node1node2node3[ansible]node4 Install Java ansible -i hosts kafka -m yum -a &quot;name=java-11-openjdk state=present&quot; -b Download and distribute software packageApache Kafka’s latest release as well as older releases can be downloaded from the below link:Apache Kafka DownloadsWe will be using Apache Kafka 3.1.0 for our setup. Once the pre-requisites are completed, proceed with the below steps: Download archive wget https://dlcdn.apache.org/kafka/3.1.0/kafka_2.13-3.1.0.tgz Distribute and extract the package on kafka nodes ansible -i hosts kafka -m unarchive -a &quot;src=kafka_2.13-3.1.0.tgz dest=/opt/&quot; -bansible -i hosts kafka -m shell -a &quot;cd /opt/; ln -s kafka_2.13-3.1.0 kafka&quot; -b Configuration setup for ZookeeperWith Kafka 2.8 Kraft was introduced to replace Zookeeper’s requirement in setting up the Kafka clusters but it is still not advised to be used in Production clusters. So, we will be going ahead with setup of zookeeper for storing Kafka’s metadata. Create zookeeper user ansible -i hosts zoo -m shell -a &quot;groupadd kafka&quot; -bansible -i hosts zoo -m shell -a &quot;useradd zookeeper -g kafka&quot; -b Create zookeeper.properties file on the ansible node dataDir=/zoodata/zookeeperclientPort=2181maxClientCnxns=0admin.enableServer=trueadmin.serverPort=4888initLimit=5syncLimit=2tickTime=2000server.1=node1:2888:3888server.2=node2:2888:3888server.3=node3:2888:3888 Distribute the zookeeper.properties file to all the zookeeper nodes ansible -i hosts zoo -m copy -a &quot;src=zookeeper.properties dest=/etc/zookeeper/conf/ mode=0644 owner=zookeeper group=kafka&quot; -b Create the required log and data directories ansible -i hosts zoo -m shell -a &quot;mkdir -p /zoodata/zookeeper&quot; -bansible -i hosts zoo -m shell -a &quot;mkdir -p /var/log/zookeeper&quot; -b /zoodata symbolises a mount point that is used to store zookeeper znodes’ data. Create the myid file on each node and add unique id on each hosts ansible -i hosts zoo -m shell -a &quot;touch /zoodata/zookeeper/myid&quot; -bansible -i hosts zoo -m shell -a &quot;echo 1 &amp;gt; /zoodata/zookeeper/myid&quot; -b --limit node1ansible -i hosts zoo -m shell -a &quot;echo 2 &amp;gt; /zoodata/zookeeper/myid&quot; -b --limit node2ansible -i hosts zoo -m shell -a &quot;echo 3 &amp;gt; /zoodata/zookeeper/myid&quot; -b --limit node3 Create a systemd file zookeeper.service with the below contents [Unit]Description=Apache ZooKeeper ServiceDocumentation=http://zookeeper.apache.orgRequires=network.targetAfter=network.target [Service]Type=simpleUser=zookeeperGroup=kafkaEnvironment=&quot;KAFKA_HEAP_OPTS=-Xmx1g -Xms1g&quot;Environment=LOG_DIR=/var/log/zookeeperExecStart=/opt/kafka/bin/zookeeper-server-start.sh /etc/zookeeper/conf/zookeeper.propertiesExecStop=/opt/kafka/bin/zookeeper-server-stop.sh /etc/zookeeper/conf/zookeeper.properties [Install]WantedBy=multi-user.target Distribute the systemd file on all hosts ansible -i hosts zoo -m copy -a &quot;src=zookeeper.service dest=/etc/systemd/system/ mode=0644&quot; -b Change Ownership of required files and directories ansible -i hosts zoo -m shell -a &quot;chown -R zookeeper:kafka /zoodata/zookeeper&quot; -bansible -i hosts zoo -m shell -a &quot;chown -R zookeeper:kafka /var/log/zookeeper&quot; -bansible -i hosts kafka -m shell -a &quot;chown -R kafka:kafka /opt/kafka/*&quot; -b Start the zookeeper service ansible -i hosts zoo -m shell -a &quot;systemctl enable zookeeper; systemctl start zookeeper&quot; Configuration setup for KafkaOnce the zookeeper quorum is achieved and we are able to test its functionality , we will be proceeding with Kafka’s configuration settings and systemd service creation for easy management of the cluster. Create Kafka user ansible -i hosts kafka -m shell -a &quot;useradd kafka -g kafka&quot; -b Create server.properties file on ansible node broker.id.generation.enable=trueport=9092controlled.shutdown.enable=truecontrolled.shutdown.max.retries=2group.max.session.timeout.ms=1800000group.min.session.timeout.ms=6000listeners=PLAINTEXT://:9092num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/kafka/kafka-logsnum.partitions=1num.recovery.threads.per.data.dir=1offsets.topic.replication.factor=3transaction.state.log.replication.factor=3transaction.state.log.min.isr=2log.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=node1:2181,node2:2181,node3:2181zookeeper.connection.timeout.ms=18000group.initial.rebalance.delay.ms=0 Distribute the server.properties file to all the kafka broker nodes ansible -i hosts kafka -m copy -a &quot;src=server.properties dest=/etc/kafka/conf/ mode=0644 owner=kafka group=kafka&quot; -b Create the log and data directories ansible -i hosts kafka -m shell -a &quot;mkdir -p /var/log/kafka&quot; -bansible -i hosts kafka -m shell -a &quot;mkdir -p /kafka/kafka-logs&quot; -b /kafka symbolises a mount point that is used to store kafka message log files. We can have multiple such mounts and each mount needs to be added in the server.properties file. Create systemd file kafka.service with the below contents [Unit]Description=Apache Kafka BrokerDocumentation=https://kafka.apache.org/documentation.htmlRequires=network.targetAfter=network.target [Service]Type=simpleUser=kafkaGroup=kafkaEnvironment=&quot;JMX_PORT=9999&quot;Environment=&quot;KAFKA_HEAP_OPTS=-Xmx4g -Xms4g&quot;Environment=LOG_DIR=/var/log/kafkaEnvironment=&quot;KAFKA_JMX_OPTS=-Djava.rmi.server.hostname=0.0.0.0 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.rmi.port=9999 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.net.preferIPv4Stack=true&quot;ExecStart=/opt/kafka/bin/kafka-server-start.sh /etc/kafka/conf/server.propertiesExecStop=/opt/kafka/bin/kafka-server-stop.shLimitNOFILE=100000TimeoutStopSec=180Restart=on-failure [Install]WantedBy=multi-user.target Distribute the systemd file on all hosts ansible -i hosts kafka -m copy -a &quot;src=kafka.service dest=/etc/systemd/system/ mode=0644&quot; -b Change the ownership of files and directories ansible -i hosts kafka -m shell -a &quot;chown -R kafka:kafka /var/log/kafka&quot; -bansible -i hosts kafka -m shell -a &quot;chown -R kafka:kafka /kafka/kafka-logs&quot; -b Start the Kafka service ansible -i hosts kafka -m shell -a &quot;systemctl enable kafka; systemctl start kafka&quot; -b Test the kafka cluster setupWe can use below set of commands to test if we are able to produce and consume messages from the newly created kafka cluster. Create test topic kafka-topics --bootstrap-server node1:9092,node2:9092,node3:9092 --create --topic testTopic --replication-factor 3 --partitions 4 Start a kafka console producer kafka-console-producer --bootstrap-server node1:9092,node2:9092,node3:9092 --topic testTopic Enter some messages on the console and then exit the console with Ctrl+C Start a kafka console consumer kafka-console-consumer --bootstrap-server node1:9092,node2:9092,node3:9092 --topic testTopic --from-beginning You should be able to see the messages that you entered in producer Now we have a fully functional Kafka cluster. All the above steps can also be completed using ansible playbook easily to setup the cluster.In the next part of this blog we will be installing MIT KDC, securing the Kafka clusters with kerberos, Enabling ACLs on topics etc.NOTE: I do not own any copyrights to mention any trademark products so any such reference should only be considered as part of educational references only." } ]
